/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth
VQLPIPSWithDiscriminator running with hinge loss.
Sanity Checking: 0it [00:00, ?it/s]
/usr/local/lib/python3.8/dist-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
  rank_zero_warn(
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name  | Type                     | Params
---------------------------------------------------
0 | loss  | VQLPIPSWithDiscriminator | 17.5 M
1 | vqvae | VQSub                    | 74.2 M
---------------------------------------------------
77.0 M    Trainable params
14.7 M    Non-trainable params
91.7 M    Total params

Sanity Checking DataLoader 0:   0%|                                              | 0/2 [00:00<?, ?it/s]dict_keys(['pixel_values', 'segmap', 'filename'])
Sanity Checking DataLoader 0:  50%|███████████████████                   | 1/2 [00:02<00:02,  2.05s/it]
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None

Sanity Checking DataLoader 0:  50%|███████████████████                   | 1/2 [00:02<00:02,  2.05s/it]dict_keys(['pixel_values', 'segmap', 'filename'])
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 6. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(

Epoch 0:   0%|                                                                 | 0/496 [00:00<?, ?it/s]dict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   0%| | 1/496 [00:05<48:52,  5.92s/it, v_num=07jy, train/aeloss_step=1.760, train/discloss_stedict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   0%| | 2/496 [00:10<42:27,  5.16s/it, v_num=07jy, train/aeloss_step=1.560, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   1%| | 3/496 [00:14<40:24,  4.92s/it, v_num=07jy, train/aeloss_step=2.840, trdict_keys(['pixel_values', 'segmap', 'filename'])
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")