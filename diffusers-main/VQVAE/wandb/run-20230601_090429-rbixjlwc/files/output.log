/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth
VQLPIPSWithDiscriminator running with hinge loss.
Sanity Checking: 0it [00:00, ?it/s]
/usr/local/lib/python3.8/dist-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
  rank_zero_warn(
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name  | Type                     | Params
---------------------------------------------------
0 | loss  | VQLPIPSWithDiscriminator | 17.5 M
1 | vqvae | VQSub                    | 74.2 M
---------------------------------------------------
77.0 M    Trainable params
14.7 M    Non-trainable params
91.7 M    Total params

Sanity Checking DataLoader 0:   0%|                              | 0/2 [00:00<?, ?it/s]dict_keys(['pixel_values', 'segmap', 'filename'])
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
Sanity Checking DataLoader 0:  50%|███████████           | 1/2 [00:02<00:02,  2.04s/it]dict_keys(['pixel_values', 'segmap', 'filename'])
Sanity Checking DataLoader 0: 100%|██████████████████████| 2/2 [00:02<00:00,  1.44s/it]
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 6. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.


Epoch 0:   0%|                                                 | 0/496 [00:00<?, ?it/s]dict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   0%| | 1/496 [00:06<50:07,  6.08s/it, v_num=jlwc, train/aeloss_step=1.750, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   0%| | 2/496 [00:10<43:03,  5.23s/it, v_num=jlwc, train/aeloss_step=2.050, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   1%| | 3/496 [00:14<40:35,  4.94s/it, v_num=jlwc, train/aeloss_step=2.830, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   1%| | 4/496 [00:19<39:24,  4.81s/it, v_num=jlwc, train/aeloss_step=3.080, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   1%| | 5/496 [00:23<38:41,  4.73s/it, v_num=jlwc, train/aeloss_step=1.800, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   1%| | 6/496 [00:28<38:10,  4.68s/it, v_num=jlwc, train/aeloss_step=1.650, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   1%| | 7/496 [00:32<37:46,  4.64s/it, v_num=jlwc, train/aeloss_step=1.440, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   2%| | 8/496 [00:36<37:28,  4.61s/it, v_num=jlwc, train/aeloss_step=1.270, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   2%| | 9/496 [00:41<37:14,  4.59s/it, v_num=jlwc, train/aeloss_step=1.200, trdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   2%| | 10/496 [00:45<37:00,  4.57s/it, v_num=jlwc, train/aeloss_step=1.270, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   2%| | 11/496 [00:50<36:48,  4.55s/it, v_num=jlwc, train/aeloss_step=1.780, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   2%| | 12/496 [00:54<36:36,  4.54s/it, v_num=jlwc, train/aeloss_step=5.780, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   3%| | 13/496 [00:58<36:24,  4.52s/it, v_num=jlwc, train/aeloss_step=14.30, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   3%| | 14/496 [01:03<36:15,  4.51s/it, v_num=jlwc, train/aeloss_step=4.800, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   3%| | 15/496 [01:07<36:08,  4.51s/it, v_num=jlwc, train/aeloss_step=2.620, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   3%| | 16/496 [01:12<36:00,  4.50s/it, v_num=jlwc, train/aeloss_step=3.010, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   3%| | 17/496 [01:16<35:54,  4.50s/it, v_num=jlwc, train/aeloss_step=1.880, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   4%| | 18/496 [01:20<35:47,  4.49s/it, v_num=jlwc, train/aeloss_step=1.980, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   4%| | 19/496 [01:25<35:40,  4.49s/it, v_num=jlwc, train/aeloss_step=1.530, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   4%| | 20/496 [01:29<35:34,  4.48s/it, v_num=jlwc, train/aeloss_step=2.580, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   4%| | 21/496 [01:34<35:28,  4.48s/it, v_num=jlwc, train/aeloss_step=3.620, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   4%| | 22/496 [01:38<35:21,  4.48s/it, v_num=jlwc, train/aeloss_step=4.790, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   5%| | 23/496 [01:42<35:16,  4.47s/it, v_num=jlwc, train/aeloss_step=4.340, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   5%| | 24/496 [01:47<35:10,  4.47s/it, v_num=jlwc, train/aeloss_step=5.100, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   5%| | 25/496 [01:51<35:04,  4.47s/it, v_num=jlwc, train/aeloss_step=7.020, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   5%| | 26/496 [01:56<34:59,  4.47s/it, v_num=jlwc, train/aeloss_step=7.460, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   5%| | 27/496 [02:00<34:53,  4.46s/it, v_num=jlwc, train/aeloss_step=2.480, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   6%| | 28/496 [02:04<34:48,  4.46s/it, v_num=jlwc, train/aeloss_step=2.000, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   6%| | 29/496 [02:09<34:42,  4.46s/it, v_num=jlwc, train/aeloss_step=1.610, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   6%| | 30/496 [02:13<34:37,  4.46s/it, v_num=jlwc, train/aeloss_step=1.760, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   6%| | 31/496 [02:18<34:32,  4.46s/it, v_num=jlwc, train/aeloss_step=2.410, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   6%| | 32/496 [02:22<34:26,  4.45s/it, v_num=jlwc, train/aeloss_step=3.600, tdict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   7%|▍     | 33/496 [02:26<34:21,  4.45s/it, v_num=jlwc, train/aeloss_step=5.320, train/discloss_step=0.636]dict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   7%|▍     | 34/496 [02:31<34:16,  4.45s/it, v_num=jlwc, train/aeloss_step=4.720, train/discloss_step=0.833]dict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   7%|▍     | 35/496 [02:35<34:11,  4.45s/it, v_num=jlwc, train/aeloss_step=4.460, train/discloss_step=0.809]dict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   7%|▍     | 36/496 [02:40<34:06,  4.45s/it, v_num=jlwc, train/aeloss_step=5.220, train/discloss_step=0.542]dict_keys(['pixel_values', 'segmap', 'filename'])
Epoch 0:   7%|▍     | 37/496 [02:44<34:01,  4.45s/it, v_num=jlwc, train/aeloss_step=4.080, train/discloss_step=0.697]dict_keys(['pixel_values', 'segmap', 'filename'])
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")